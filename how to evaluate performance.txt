How to evaluate the performance of unsupervised learning?

Methods:

1. Transfer unsupervised learning to supervised learning.
Choose some instances randomly from the original dataset (such as choose 10 percent of total number of original data, and including repeated and unrepeated instances). Then give them labels manually, and then divide them into training data and test data. Apply the algorithm to training data and test data and to see the performance, when the performance is good (use standard such as Precision……), then apply the algorithm to the all data.

2. Run unsupervised learning first, then verify the results
Choose some instances randomly, and give them labels manually and save to another file. Run the unlabeled original data, then compare the algorithm’s result to data which has been labeled.

3. Assumption: among our data, the duplicate data accounts for a small portion
Run the unlabeled, original data first, then we will get the results about what are duplicate data. Then we pay attention to those who have been predicted duplicate. We compare the similarity of fields of different data(such as regular expression, I assume that a lot of information will be same or similar), to improve the performance, we can set different weights to different fields according to their importance.

4. Try Ensemble learning
Use some unsupervised learning as base learner so that combine the results from different methods.

5. I don't know if this method could be realized, similiar to the second type, but add how to improve the result part
firstly, choose a suitable algorithm, such as kmeans, and apply the algorithm to all the original, unlabeled data, and wait for the clustering results;
secondly, in the results, there are several types according to all the features included. From all these types generated by algorithms, we can choose some instances and give them "correct" labels according to our knowledge;
thirdly, we can calculate the accuracy, such as: 
    "the number of correct classified instances among the instances checked / the number of all the instances checked"
to evaluate the performance of the algorithm, and we can set up the threshold, if the accuracy is higher than it, we can stop the algorithm; but if the accuracy is lower than, we can continue training. In the iteration of training, there are different ways, 
        the first is very simple, just to run the algorithm again to see if there is any diference;
        the second is is there any methods that add manual labels information to the training process? then in the next training process, the algorithm may get some more information. (Like in kmeans, the algorithm calculates the distance and classfy every time, then calculate the new center according to the same type instances, then calculate and classify again. Is there any methods to "revise" some instances before calculating the new centers?)
        
        
Try experiments:
1. Try the third method and the second method first. Use this method to evaluate the result of previous result and compare the difference between the two evaluation;
2. If the result is not very accurate, how to improve the method? Can the fifth method to be used? or just use the first method, transforming the unsupervised method to supervised method?
