1.15
Work I have done:
1. Fixed bugs of dedupe_join_files and run the program on the fibre_file and neighbourly_file, these are findings:
	1. In general, I think the result looks good, I checked some records which are classified "same record", most of them are from the same user;
	2. The program not only fond the same users in different files and also checked within the same file again and also found some same records within the same file again;
	3. The time consumed reduced a lot (before labelling, and after labelling), I assumed this is because I transformed the type of field to Set, I will check the assumption.
2. Write the evaluate_combined file so that I can know the specific performance of this training. I haven't finished this part, will update the file after finish it.
3. Log in database successfully.

1.14
Work I have done:
1. update the combine_same_records program and reduced running time;
2. revised the format of the final result file (fibre_file and neighbourly_file), add country column in the neighbourly so that makes it easier to compare among files; Also change the type of fields, all the fields are transformed into "Set" so that the combined rows can also be compared appropriately;
3. Ran the combine_same_records on the fibre_result and neighbourly_result, everything seems ok;
4. Wrote dedupe_join_files, which is used when comparing multiple files, currently in test status.

1.13
Work I have done:
1. When I run the model getting from fibre_file on the neighbourly_file, I found that neighbourly_file miss one column: Country; so I tried to remove the country from fibre_file and add the country column to the neighbourly file. And I found that the algorithm seems can not support such operation, it can not finish the process.
2. I started to do the matching between files, I planned to try this method first: run the single file first, and combine the same records to one record, then combine with other files. So I write a file to combine the same records to one record, still have several bugs, I will update it late today.

1.10
Work I have done:
1. Update all the fibre related files to make sure all the experiments can be tried;
2. I separate the email field into eaddress and domain of the fibre file and run the following algorithm. Because in the fibre file, the not null rate of email is very high, about 0.9, so when separate email field, the first part of email address works very good.
3. I tried train part features(first_name, last_name, eaddress, domain and phone_number) and all features in the fibre_file, because of the high not null rate, both the accurate is more than 0.9, and when using all the features, the algorithm found more effective duplicates.
4. I want to try to use the training model trained from fibre_file on neighbourly_file to see what the result will be. because the training model of the fibre_file may work better than neighbourly's model. I will update the result, after finish this experiment.

1.09
Work I have done:
1. Change the order of features and run the active learning algorithm on the neighbourly_file, According to the result, the features which are given weights are also first_name and address_line, however after training about more than 750 instances( 46 positive instances), the correct_classified rate is about 0.38, higher than the previous result;
2. Update the script about analyzing the neighbourly_file;

1.08
Work I have done:
1. I tried to separate the email address in the neighbourly_file (address and domain) so that when comparing the contents, the algorithm can neglect the difference of domain. Actually, according to the result, I think separating email address can improve.
2. I run the active learning on the neighbourly_file and trained about more than 450 instances (52 positive instances), and according to the result, I found it has improved a little bit, however, has some strange mistakes which I can not identify the reason for the classification. I will try to use a different order of features to see how dedupe train instances.
3. To make the future matching work easier, I add a label for every record to record the file they come from.

1.07
Work I have done:
1. Yesterday, I have finished using 5 features on the neighbourly_file, so I firstly run the evaluation code on the five features result, the result is bad too,0.20900(19879/95112), and when I look at the final result file, I found that the active learning algorithm set much weights on the last_name and address_line features, which is not suitable and sufficient. So I decided to train the data again, and pay attention to the weights shown during the labelling process to avoid unreasonable weights. And I will also try to revise email contents before training, reduce the @ and after @ contents.
2. I tried to review the matrix_file and to see if there are any methods to separate the company and personal accounts. I think it is complicated, I found that several emails may belong to Stuff staffs, such as sales person or customer servicesw people, these emails will repeat several times. I will alos focus on the file and to see if some methods can be used to identify different users.

1.06
Work I have done:
1. Fixing the evaluate script so that I can use evaluate file to evaluate the performance of dedupe_neighbourly result (with four features: first_name, last_name, email, phone_number), The correct rate of the four features is 0.322127 (272782/846813)
2. Try to use five features (add the "address_line") to run the algorithm and get 21070 duplicate sets and haven’t finished the evaluation process.
Probelm I met:
1. Running the experiments costs too much running time (especially neighbourly_file and matrix_file). I am thiniking about if the running process can be improved to reduce some running time? 

12.20
Work I have done:
1. Fixed the bug why confidence score does not show in the output file;
2. In fibre_file, run different features chosen on active learning algorithm, one uses four features:first_name, last_name, email, ohone_number; the other uses all the features. After finishing the two processes, I used the evaluate code to see the performance of the two experiments:
	The performance of the two experiments are almost the same, the accurate rate is about 93%, and when using regular expression to compare the two results, the four features seems better(I am still thinking about why), I think for this dataset, the reason why the two results are similar is that:
		1. use the "key factor" to train data;
		2. the NOT NULL rate is very high (first_name:99.9%;last_name:99.9%;email:98.7%;phone_number:87.1%)
3. I also tried the experiment on the neighbourly dataset, because of the large dataset, just finished the whole process, haven't finished the evaluate process, I will update the result after finish that.

12.19
Work I have done:
1. Revised the three phone_number columns into one phone_number column in the fibre_file, and run dedupe on the fibre data with different features chosen (one used all the features; the other used four features: first_name, last_name, email, phone_number). From the process, when the number of features used reduces, the running time also reduces.
2. However, I has not run the result. I have changed phone number's type into "Set", and some method(canonicalize()) can not accept this type when calculating the distance between different contents, so to make the process run, I rewrite dedupe's code. Now the whole process can run, but the output file also has a bug: the confidence score and canonicalize_contents are not shown and I am figuring out this problem now. After I fix this bug, I will keep running different number of features with dedupe to see the result.

12.18
Work I have done:
1. When I evaluating the valid of phone_number, I found that we can combine the three columns about phone numbers(phone_1, phone_2, phone_3) as one column, so that we can improve the non-null proportion and validity of the data. And in dedupe part, we can set the phone_number "Set" type, then they can handle list as the content of field. After combining al the phone number columns, the validity has improved a little:
	1. non-null proportion decrease by about 2%;
	2. validity probability (only contains numbers, no length restriction because of different countries) has increased by about 3%
2. Caculate the non-null proportion of each field.


12.17
Work I have done:
1. Today I tried to use pandas.DataFrame and regular expression to analyze the valid of each column contents, still doing experiments, not finished
Problems I met:
1. I first tried to use DataFrame to analyze all the columns, and I found that all the columns are assigned to object type, including the phone, postcode, then I found that there are various formats of contents users provide. Among these different formats, some are invalid information, however, some information is "about valid" like (02378298534wrk), for these mistakes, I am thinking about how to handle these information.
2. According to yesterday's iscussion, I am thinking about that we need to preprocess original data firstly, so that we can know the validity of this column data, then we may improve the result later. So I also tried regular expression to check the validity of the data, such as, if the column is about email, it must contain only one"@"......However, I found that the reality is always complicated than our imagines, such as xxxx.xxxx#gmail.com, it is wrong accoring to the regex, however we can also extract useful information from the mistakes. i am thinking about if we can extract information from these mistakes and how to. Now I have no idea...

12.16
Work I have done:
1. In the morning, I had weekly meeting with Bryan, and made the plan for this week, dig into the data I have, and try to know about the validity of different columns in the data, then try different columns(features) combination to run active learning ans see what the result will be. I also learned that data science needs to know a lot about the data(where does it come from, what it should be, then we can use the data as much as possible), at the same time, know about the science to deal with the data!!!
2. Used pandas.DataFrame.describe to assess the validity of each column, when transferring the type of column, bugs found and fixed. (we can found how users fill in these fields by seeing bugs, if bugs are small mistakes, we can fix them to improve the validity.)

12.13
Work I have done:
1. Start running active learning algorithm on the matrix_file data and neighbourly_file data, also met zerodivisionerror, fixed it after transfer empty to null;
Problems I met:
1. There are some unexpected situations in the data, such as in the matrix_file, there are many records' last_name looks like company name, and I assume these data are test data wrongly during labelling these data, which leads to the bad result of active learning(just look at the result CSV file). I think when processing data, I also need to know some information about Stuff services to avoid basic information. 
2. In the matrix_file and neighbourly_file, there are many records who misses some information(no name, no email, no phone numbers, especially misses all these key information), which makes difficult to identify the similarity between different records, I want to some other methods to see if can improve the result(such as combine with other files first then classify these data).

12.12
Work I have done:
1. Fixing bugs of reading data from matrix_file, now Matrix_file has been read, there are 633,537 records read from the file;
2. Finish reading neighbourly_file, there are 884,524 records read from the file.

12.11
Work I have done:
1. I am still focusing on read data from matrix file, today I figured how to extract data when the teo records are combined with each other, like this format:{'unique_id|"first_name"|"last_name"|"address_line"|"suburb"|"city"|"postcode"|"country"|"dob"|"email"|"phone_1"|"phone_2"|"phone_3"': '3323622|"PHILIP & GLENDA"|"MCDONNELL & STEWART"|"30 MILLSTREAM DRIVE"|"NORTHWOOD"|"CHRISTCHURCH 8051"|"8051"|"NEW ZEALAND"||"GLENDASTEWART25@HOTMAIL.CO', None: ['|||\n3323623"|"KEVIN"|"HENRY"|"268 MARSDEN ROAD"||"GREYMOUTH 7805"|"7805"|"NEW ZEALAND"|||||']}
and now I am figuring out how to extract data validly when user wrote some special information (such as I use "|" as delimiter, however some users has "|" in their field contents).
Till now, there are 633,543 records in the matrix file and I have extracted 633,535 records, I hope to finish reading the file tomorrow.
Problems I met:
1. There are so many different "abnormal" data in the original file, I should be more careful so that I won't miss any data.

12.10
Work I have done:
1. I am still figuring out extracting data from the matrix file. Try to use different methods. The new progress have uploaded: try.py
2. I haven't finished adding documentation yet, still adding...

12.09
Work I have done:
1. I am focusing on reading and extracting data from the other two files, currently, I am focusing on the matrix file. the size of the file is much bigger than the fibre file, and with an increasing number of records, users have a more different format of contents which leads to more situations. I have learned that users can write down any contents, including some special punctuation marks, so if we need to do some operations according to some contents, we need to preprocess users' contents first.
2. Currently, I still fixing a bug when reading the matrix file: 
	some users' record like this format:
		{'unique_id|"first_name"|"last_name"|"address_line"|"suburb"|"city"|"postcode"|"country"|"dob"|"email"|"phone_1"|"phone_2"|"phone_3"': '3323622|"PHILIP & GLENDA"|"MCDONNELL & STEWART"|"30 MILLSTREAM DRIVE"|"NORTHWOOD"|"CHRISTCHURCH 8051"|"8051"|"NEW ZEALAND"||"GLENDASTEWART25@HOTMAIL.CO', None: ['|||\n3323623"|"KEVIN"|"HENRY"|"268 MARSDEN ROAD"||"GREYMOUTH 7805"|"7805"|"NEW ZEALAND"|||||']}
	his record is combined with another user's record, the bug is that in the data I read from the original file, the second data(KEVIN's) is not extracted from the file.

12.06
Work I have done:
1. I am writing code about extract and preprocess data from another two original files. Still not finished. There are several situations,such as some special punctuation marks (\n) and some single records account for multiple cells. I am figuring these problems out. The newest code has been uploaded: dedupe_three.py
2. I am also revising the code of reading the first orinal file to make the file can be used on the three files

12.05
Work I have done:
1. Fix bugs in the evaluate_re.py, when comparing the similarity between email addresses, remove the domain part, just compare the name of the address in case of different  
2. Using active learning and try different number of training instances to see if the result has some improvement.(last time, I used about 25 instances each for positive and negative, this time i tried about 50 instances; the correct rate(according to my evaluate file)has some improvement, using string metrics, the correct rate increase from 0.89 to 0.93; using regular expression, the correct rate increases from 0.77 to 0.79)
3. Started to write code to preprocess the another two files, hasn't finished, the newest code has uploaded.
Problems I met:
1. Has two bugs not checked fixed in reading another two original files:
		1. When a record has several cells, I just ignore the second line, need to check the number of instances is equal to the number of records in the  original file;
		2. When there is “\n” in user’s writing, how to extract the whole record from the original file

12.04
Work I have done:
1. I checked Dedupe's documentation and found that in the active learning, they already use string metrics to calculate the similarity between different records, so I think maybe we can use another method to check the result, so I used regular expression to calculate similarity. use re.match() and re.search() method to find to what degree these tewo records are similar.
2. the result from regular expression is higher than result from string metrics, I adjusted the weights of different fields, the regular expression result is about 0.95, while the string metrics result is about 0.89.
3. Among the regular expression results I found that there is some records labeled "insimilar"(correct rate lecc than 0.5)should be similar, i revides code on the email field,(just extract the address name without the domain),has bugs, still fixing...the latest code uploaed as evaluate_re.py

12.03
Work I have done:
1. Finish using the second method to evaluate the result coming from the active learning:
    I extracted all the similiar records and used String similarity metrics with different weights to compare records which are classified same, then use the ratio of correctly classified pairs as the final performance of the active learning, accoring to the current method and the result, the correct_pair_rate is about 0.925.
    the update code has been uploaded: evaluate.py
Problems I met:
1. When dealing with the same record blocks with more than two records, it is more complicated to compare records than two-records blocks. There are several situations inside blocks with more than 2 records: all of them are correct; several of them are correct; none of them is correct......Now I just separate the big blocks into 2-records block and compare the two records every time, then I use the correct_pair_rate in replace of correct_record_rate to evaluate the performance.

12.02
Work I have done:
1. Continue writing Python code for evaluating the result of the active learning. Today i mainly use the second method i mentioned in the txt file: use the result as the data, and use String metrics method to evaluate the similarity between differet records, the specific code uploaded to the evaluate.py file;
2. About the specific method, I first use single field, then use combined fields
Problems I met:
1. There are two types of same records, the first one is the block has only two "same" records, the second has more than two same records, about the latter one, I am thinking about how to evaluate them with String metrics.

11.29
Work I have done:
1. set up the plan for next progress:
  I have run active learning and got the result, about how to evaluate and improve the result, still needs to do, here is the experiment plan:
    1. Try the third method and the second method first. Use this method to evaluate the result of previous result and compare the difference between the two evaluation;
    2. If the result is not very accurate, think about and try different methods to improve the result? Can the fifth method to be used? or just use the first method, transforming the unsupervised method to supervised method?
2. Started to write python script about evaluate the result of the active learning, not finished, the newest code is uploaded: evaluate.py

11.28
Work I have done:
1. Also googled about the effectiveness of unsupervised learning, and add one more method to the question, also update on the txt file, here is the link: https://github.com/Chen-Jiang/internship19/blob/master/how%20to%20evaluate%20performance.txt
2. See context about string metrics technology, see difflib, Levenshtein, Sørensen, and Jaccard similarity values for two string, and also write some small experiments using these methods to see the result, the experiments are listed in the String Similarity/env_1 folder.

11.27
Work I have done:
1. Attend the Intro to TDD meeting
2. Google and write down how to evaluate performance of unsupervised learning, below is my thoughts(I will also upload the file to git):
Methods:

1. Transfer unsupervised learning to supervised learning.
Choose some instances randomly from the original dataset (such as choose 10 percent of total number of original data, and including repeated and unrepeated instances). Then give them labels manually, and then divide them into training data and test data. Apply the algorithm to training data and test data and to see the performance, when the performance is good (use standard such as Precision……), then apply the algorithm to the all data.

2. Run unsupervised learning first, then verify the results
Choose some instances randomly, and give them labels manually and save to another file. Run the unlabeled original data, then compare the algorithm’s result to data which has been labeled.

3. Assumption: among our data, the duplicate data accounts for a small portion
Run the unlabeled, original data first, then we will get the results about what are duplicate data. Then we pay attention to those who have been predicted duplicate. We compare the similarity of fields of different data(such as regular expression, I assume that a lot of information will be same or similar), to improve the performance, we can set different weights to different fields according to their importance.

4. Try Ensemble learning
Use some unsupervised learning as base learner so that combine the results from different methods.

11.26
Work I have done:
1. do some TDD exercises, please see the PyTDDProject folder.
2. finish the TDD notes, please go to PyTDDProject/TDD notes.txt
3. google about how to evaluate the result of machine learning algorithm

11.25
Work I have done:
1. use google to learn about TDD, what is TDD, and how to implement TDD
2. find several examples from google to practice using TDD when programming, language is Python

11.22
Work I have done:
1. fix the bug and apply the algorithm to the original dataset, get some results
2. fix some code, such as add the low() to make sure identify the same contents more precisely
Problems I met:
1. How to evaluate the result of the algorithm?

11.21
Work I have done:
1. see dedupe's document and have better understanding of algorithm;
2. run yesterday's code on sample data and original data;
3. when running code on original there are some bugs, and worked on fixing bugs, such as ZeroDivisionError: float division; Still fixing...not finished
Problems I met:
1. How to evaluate the performance of the result? In the provided examples, there is a file with manual labels so that the results can be evaluated. How can we evaluate our results?

11.20
Work I have done:
1. I planned to experiment the algorithm in csv-example folder provided by dedupe, get familiar with the example algorithm;
2. I have preprocessed the raw data yesterday, and today based on yester's worl, I transfered the OrderedDict to teh regular dictionary format so that the data can be dealed with by the algorithm;
3. According to the algorithm, after preprocessing the raw data, try to use active learning to train the dataset;
Problems I met:
1. do not have good understanding of the algorithm provided by the example, expecially about the add cluster part (in this part, several columns are added to the input file), so still need to work more on the experiment.

11.19
I planned to process our data using deduce tool, use one of the dataset(fibre), however found the CSV file is not a standard CSV file, so preprocess the data first and make sure the file is standard csv file, also adjust the format of the data, correct some errors, such as some contents are written in a same cell.
Also read the adjusted csv file and create a dictionary of records
