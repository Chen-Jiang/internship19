12.16
Work I have done:
1. In the morning, I had weekly meeting with Bryan, and made the plan for this week, dig into the data I have, and try to know about the validity of different columns in the data, then try different columns(features) combination to run active learning ans see what the result will be. I also learned that data science needs to know a lot about the data(where does it come from, what it should be, then we can use the data as much as possible), at the same time, know about the science to deal with the data!!!
2. Used pandas.DataFrame.describe to assess the validity of each column, when transferring the type of column, bugs found and fixed. (we can found how users fill in these fields by seeing bugs, if bugs are small mistakes, we can fix them to improve the validity.)

12.13
Work I have done:
1. Start running active learning algorithm on the matrix_file data and neighbourly_file data, also met zerodivisionerror, fixed it after transfer empty to null;
Problems I met:
1. There are some unexpected situations in the data, such as in the matrix_file, there are many records' last_name looks like company name, and I assume these data are test data wrongly during labelling these data, which leads to the bad result of active learning(just look at the result CSV file). I think when processing data, I also need to know some information about Stuff services to avoid basic information. 
2. In the matrix_file and neighbourly_file, there are many records who misses some information(no name, no email, no phone numbers, especially misses all these key information), which makes difficult to identify the similarity between different records, I want to some other methods to see if can improve the result(such as combine with other files first then classify these data).

12.12
Work I have done:
1. Fixing bugs of reading data from matrix_file, now Matrix_file has been read, there are 633,537 records read from the file;
2. Finish reading neighbourly_file, there are 884,524 records read from the file.

12.11
Work I have done:
1. I am still focusing on read data from matrix file, today I figured how to extract data when the teo records are combined with each other, like this format:{'unique_id|"first_name"|"last_name"|"address_line"|"suburb"|"city"|"postcode"|"country"|"dob"|"email"|"phone_1"|"phone_2"|"phone_3"': '3323622|"PHILIP & GLENDA"|"MCDONNELL & STEWART"|"30 MILLSTREAM DRIVE"|"NORTHWOOD"|"CHRISTCHURCH 8051"|"8051"|"NEW ZEALAND"||"GLENDASTEWART25@HOTMAIL.CO', None: ['|||\n3323623"|"KEVIN"|"HENRY"|"268 MARSDEN ROAD"||"GREYMOUTH 7805"|"7805"|"NEW ZEALAND"|||||']}
and now I am figuring out how to extract data validly when user wrote some special information (such as I use "|" as delimiter, however some users has "|" in their field contents).
Till now, there are 633,543 records in the matrix file and I have extracted 633,535 records, I hope to finish reading the file tomorrow.
Problems I met:
1. There are so many different "abnormal" data in the original file, I should be more careful so that I won't miss any data.

12.10
Work I have done:
1. I am still figuring out extracting data from the matrix file. Try to use different methods. The new progress have uploaded: try.py
2. I haven't finished adding documentation yet, still adding...

12.09
Work I have done:
1. I am focusing on reading and extracting data from the other two files, currently, I am focusing on the matrix file. the size of the file is much bigger than the fibre file, and with an increasing number of records, users have a more different format of contents which leads to more situations. I have learned that users can write down any contents, including some special punctuation marks, so if we need to do some operations according to some contents, we need to preprocess users' contents first.
2. Currently, I still fixing a bug when reading the matrix file: 
	some users' record like this format:
		{'unique_id|"first_name"|"last_name"|"address_line"|"suburb"|"city"|"postcode"|"country"|"dob"|"email"|"phone_1"|"phone_2"|"phone_3"': '3323622|"PHILIP & GLENDA"|"MCDONNELL & STEWART"|"30 MILLSTREAM DRIVE"|"NORTHWOOD"|"CHRISTCHURCH 8051"|"8051"|"NEW ZEALAND"||"GLENDASTEWART25@HOTMAIL.CO', None: ['|||\n3323623"|"KEVIN"|"HENRY"|"268 MARSDEN ROAD"||"GREYMOUTH 7805"|"7805"|"NEW ZEALAND"|||||']}
	his record is combined with another user's record, the bug is that in the data I read from the original file, the second data(KEVIN's) is not extracted from the file.

12.06
Work I have done:
1. I am writing code about extract and preprocess data from another two original files. Still not finished. There are several situations,such as some special punctuation marks (\n) and some single records account for multiple cells. I am figuring these problems out. The newest code has been uploaded: dedupe_three.py
2. I am also revising the code of reading the first orinal file to make the file can be used on the three files

12.05
Work I have done:
1. Fix bugs in the evaluate_re.py, when comparing the similarity between email addresses, remove the domain part, just compare the name of the address in case of different  
2. Using active learning and try different number of training instances to see if the result has some improvement.(last time, I used about 25 instances each for positive and negative, this time i tried about 50 instances; the correct rate(according to my evaluate file)has some improvement, using string metrics, the correct rate increase from 0.89 to 0.93; using regular expression, the correct rate increases from 0.77 to 0.79)
3. Started to write code to preprocess the another two files, hasn't finished, the newest code has uploaded.
Problems I met:
1. Has two bugs not checked fixed in reading another two original files:
		1. When a record has several cells, I just ignore the second line, need to check the number of instances is equal to the number of records in the  original file;
		2. When there is “\n” in user’s writing, how to extract the whole record from the original file

12.04
Work I have done:
1. I checked Dedupe's documentation and found that in the active learning, they already use string metrics to calculate the similarity between different records, so I think maybe we can use another method to check the result, so I used regular expression to calculate similarity. use re.match() and re.search() method to find to what degree these tewo records are similar.
2. the result from regular expression is higher than result from string metrics, I adjusted the weights of different fields, the regular expression result is about 0.95, while the string metrics result is about 0.89.
3. Among the regular expression results I found that there is some records labeled "insimilar"(correct rate lecc than 0.5)should be similar, i revides code on the email field,(just extract the address name without the domain),has bugs, still fixing...the latest code uploaed as evaluate_re.py

12.03
Work I have done:
1. Finish using the second method to evaluate the result coming from the active learning:
    I extracted all the similiar records and used String similarity metrics with different weights to compare records which are classified same, then use the ratio of correctly classified pairs as the final performance of the active learning, accoring to the current method and the result, the correct_pair_rate is about 0.925.
    the update code has been uploaded: evaluate.py
Problems I met:
1. When dealing with the same record blocks with more than two records, it is more complicated to compare records than two-records blocks. There are several situations inside blocks with more than 2 records: all of them are correct; several of them are correct; none of them is correct......Now I just separate the big blocks into 2-records block and compare the two records every time, then I use the correct_pair_rate in replace of correct_record_rate to evaluate the performance.

12.02
Work I have done:
1. Continue writing Python code for evaluating the result of the active learning. Today i mainly use the second method i mentioned in the txt file: use the result as the data, and use String metrics method to evaluate the similarity between differet records, the specific code uploaded to the evaluate.py file;
2. About the specific method, I first use single field, then use combined fields
Problems I met:
1. There are two types of same records, the first one is the block has only two "same" records, the second has more than two same records, about the latter one, I am thinking about how to evaluate them with String metrics.

11.29
Work I have done:
1. set up the plan for next progress:
  I have run active learning and got the result, about how to evaluate and improve the result, still needs to do, here is the experiment plan:
    1. Try the third method and the second method first. Use this method to evaluate the result of previous result and compare the difference between the two evaluation;
    2. If the result is not very accurate, think about and try different methods to improve the result? Can the fifth method to be used? or just use the first method, transforming the unsupervised method to supervised method?
2. Started to write python script about evaluate the result of the active learning, not finished, the newest code is uploaded: evaluate.py

11.28
Work I have done:
1. Also googled about the effectiveness of unsupervised learning, and add one more method to the question, also update on the txt file, here is the link: https://github.com/Chen-Jiang/internship19/blob/master/how%20to%20evaluate%20performance.txt
2. See context about string metrics technology, see difflib, Levenshtein, Sørensen, and Jaccard similarity values for two string, and also write some small experiments using these methods to see the result, the experiments are listed in the String Similarity/env_1 folder.

11.27
Work I have done:
1. Attend the Intro to TDD meeting
2. Google and write down how to evaluate performance of unsupervised learning, below is my thoughts(I will also upload the file to git):
Methods:

1. Transfer unsupervised learning to supervised learning.
Choose some instances randomly from the original dataset (such as choose 10 percent of total number of original data, and including repeated and unrepeated instances). Then give them labels manually, and then divide them into training data and test data. Apply the algorithm to training data and test data and to see the performance, when the performance is good (use standard such as Precision……), then apply the algorithm to the all data.

2. Run unsupervised learning first, then verify the results
Choose some instances randomly, and give them labels manually and save to another file. Run the unlabeled original data, then compare the algorithm’s result to data which has been labeled.

3. Assumption: among our data, the duplicate data accounts for a small portion
Run the unlabeled, original data first, then we will get the results about what are duplicate data. Then we pay attention to those who have been predicted duplicate. We compare the similarity of fields of different data(such as regular expression, I assume that a lot of information will be same or similar), to improve the performance, we can set different weights to different fields according to their importance.

4. Try Ensemble learning
Use some unsupervised learning as base learner so that combine the results from different methods.

11.26
Work I have done:
1. do some TDD exercises, please see the PyTDDProject folder.
2. finish the TDD notes, please go to PyTDDProject/TDD notes.txt
3. google about how to evaluate the result of machine learning algorithm

11.25
Work I have done:
1. use google to learn about TDD, what is TDD, and how to implement TDD
2. find several examples from google to practice using TDD when programming, language is Python

11.22
Work I have done:
1. fix the bug and apply the algorithm to the original dataset, get some results
2. fix some code, such as add the low() to make sure identify the same contents more precisely
Problems I met:
1. How to evaluate the result of the algorithm?

11.21
Work I have done:
1. see dedupe's document and have better understanding of algorithm;
2. run yesterday's code on sample data and original data;
3. when running code on original there are some bugs, and worked on fixing bugs, such as ZeroDivisionError: float division; Still fixing...not finished
Problems I met:
1. How to evaluate the performance of the result? In the provided examples, there is a file with manual labels so that the results can be evaluated. How can we evaluate our results?

11.20
Work I have done:
1. I planned to experiment the algorithm in csv-example folder provided by dedupe, get familiar with the example algorithm;
2. I have preprocessed the raw data yesterday, and today based on yester's worl, I transfered the OrderedDict to teh regular dictionary format so that the data can be dealed with by the algorithm;
3. According to the algorithm, after preprocessing the raw data, try to use active learning to train the dataset;
Problems I met:
1. do not have good understanding of the algorithm provided by the example, expecially about the add cluster part (in this part, several columns are added to the input file), so still need to work more on the experiment.

11.19
I planned to process our data using deduce tool, use one of the dataset(fibre), however found the CSV file is not a standard CSV file, so preprocess the data first and make sure the file is standard csv file, also adjust the format of the data, correct some errors, such as some contents are written in a same cell.
Also read the adjusted csv file and create a dictionary of records
